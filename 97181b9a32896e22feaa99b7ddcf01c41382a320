{
  "comments": [
    {
      "key": {
        "uuid": "1453cb60_84ae623d",
        "filename": "nodepool/driver/azurestate/adapter.py",
        "patchSetId": 6
      },
      "lineNbr": 65,
      "author": {
        "id": 16068
      },
      "writtenOn": "2021-05-21T18:22:02Z",
      "side": 1,
      "message": "Since this runs synchronously I wonder what\u0027s the expected runtime of those api calls. I know that quite some api calls on our openstack clouds are in the order of seconds which is likely too slow to have it inside the critical path in the state machine.\n\nSo I think in order to really gain speed here in future we likely will need to move all remote api calls to the threadpool like the key scanning. I think a shared threadpool for all those actions might make sense.\n\nI think this would mimick the asyncio approach quite closely without giving up threads.",
      "range": {
        "startLine": 63,
        "startChar": 0,
        "endLine": 65,
        "endChar": 0
      },
      "revId": "97181b9a32896e22feaa99b7ddcf01c41382a320",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9487a3ec_61b97b53",
        "filename": "nodepool/driver/azurestate/adapter.py",
        "patchSetId": 6
      },
      "lineNbr": 65,
      "author": {
        "id": 1
      },
      "writtenOn": "2021-05-21T20:55:31Z",
      "side": 1,
      "message": "A central premise of the nodepool launcher design is that we intentionally want to serialize API calls to a particular cloud; they should be much shorter than the actual work of launching nodes.  We have done quite a lot of work in the existing openstack driver (much of which is now in openstacksdk) to ensure this.  That\u0027s what the whole \"taskmanager\" thing is about.  A given provider-pool makes a single API call at a time.  The taskmanager has a thread that actually executes those calls; the rest of the threads are simply to keep track of the state of the individual instance launches.\n\nA big part of the idea here is to keep that behavior, but eliminate the overhead of context switching between threads.\n\nI\u0027m not sure that we should try to paralellize more than we already are.  It makes rate limiting difficult to calculate, and even with all the limitations we have, Nodepool has caused at least two full-scale service outages on public OpenStack clouds (these are at least the ones we know about).\n\n(Also, depending on exactly what openstack is doing in those api requests, there\u0027s a really good chance than parelellizing won\u0027t actually make it better.  Might make it worse.  Certainly worth looking into.)",
      "revId": "97181b9a32896e22feaa99b7ddcf01c41382a320",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    }
  ]
}