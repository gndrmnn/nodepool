{
  "comments": [
    {
      "key": {
        "uuid": "5f7c97a3_3d6ea1fe",
        "filename": "doc/source/configuration.rst",
        "patchSetId": 10
      },
      "lineNbr": 493,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "I would not list all max-* values here. The quota code can be extended easily to support further quota types like max-volume-gb, max-volumes, max-ports, max-floatingips, etc. So I\u0027d prefer to have this documented in a generic way so this doesn\u0027t get outdated on any new quota type we support.\n\nWhat about \u0027Instead, only check against the max values of this pool...\u0027?",
      "range": {
        "startLine": 493,
        "startChar": 4,
        "endLine": 493,
        "endChar": 50
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_f86ef762",
        "filename": "doc/source/configuration.rst",
        "patchSetId": 10
      },
      "lineNbr": 493,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "5f7c97a3_3d6ea1fe",
      "range": {
        "startLine": 493,
        "startChar": 4,
        "endLine": 493,
        "endChar": 50
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_d88bfb66",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 308,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "nit: While you\u0027re at it we may standardize this to \u0027Predicted remaining provider quota...\u0027.",
      "range": {
        "startLine": 308,
        "startChar": 48,
        "endLine": 308,
        "endChar": 54
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_b8b71fd8",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 308,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "5f7c97a3_d88bfb66",
      "range": {
        "startLine": 308,
        "startChar": 48,
        "endLine": 308,
        "endChar": 54
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_d85d3b17",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 333,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "This should not be short-circuited. This makes the max-* settings of the pool essentially ignored. The needed_quota is needed also for calculating the configured pool max values below.\n\nThis also doesn\u0027t seem to be covered by the test case. So you need to also test if the pool max settings are obeyed if ignore_provider_quota is True.",
      "range": {
        "startLine": 331,
        "startChar": 0,
        "endLine": 333,
        "endChar": 73
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_58e4cba8",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 333,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "I think I see what you mean. Let me add a test case to validate as you suggest and I\u0027ll adjust this and validate that it all still works right.",
      "parentUuid": "5f7c97a3_d85d3b17",
      "range": {
        "startLine": 331,
        "startChar": 0,
        "endLine": 333,
        "endChar": 73
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_ecbb98e1",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 333,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:45:32Z",
      "side": 1,
      "message": "Hmm, no - removing this causes it to totally fail. I\u0027ve added tests in the follow up to show how this is working correctly because doing what is currently done makes the cloud quota totally irrelevant - nodepool creates a quota, subtracts the nodes it already has in zk from that quota, then renders the result. What\u0027s in the cloud already doesn\u0027t matter to nodepool.",
      "parentUuid": "5f7c97a3_58e4cba8",
      "range": {
        "startLine": 331,
        "startChar": 0,
        "endLine": 333,
        "endChar": 73
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_6c754833",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 333,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:48:31Z",
      "side": 1,
      "message": "If I\u0027ve missed something obvious or if you can suggest how this could be done better to take the concern into account, please let me know!",
      "parentUuid": "5f7c97a3_ecbb98e1",
      "range": {
        "startLine": 331,
        "startChar": 0,
        "endLine": 333,
        "endChar": 73
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_cc257c18",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 10
      },
      "lineNbr": 333,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T17:00:20Z",
      "side": 1,
      "message": "I still think this should be like this:\n\n    def hasProviderQuota(self, node_types):\n        needed_quota \u003d QuotaInformation()\n\n        for ntype in node_types:\n            needed_quota.add(\n                self.manager.quotaNeededByNodeType(ntype, self.pool))\n\n        if not self.pool.ignore_provider_quota:        \n            cloud_quota \u003d self.manager.estimatedNodepoolQuota()\n            cloud_quota.subtract(needed_quota)\n\n            if not cloud_quota.non_negative():\n                return False\n\n        # Now calculate pool specific quota. Values indicating no quota default\n        # to math.inf representing infinity that can be calculated with.\n        pool_quota \u003d QuotaInformation(cores\u003dself.pool.max_cores,\n                                      instances\u003dself.pool.max_servers,\n                                      ram\u003dself.pool.max_ram,\n                                      default\u003dmath.inf)\n        pool_quota.subtract(needed_quota)\n        return pool_quota.non_negative()\n\nOtherwise you don\u0027t count the configured pool quota. We should only ignore the cloud quota calculation not the pool quota calculation. Note that this method checks if the requested nodeset could fit into the provider at all regardless of the current usage (so it can decline it because it will never ever be able to fulfill the request). So if you have a pool with max-server 5 and you get a node request with 6 servers this method will deny this.\n\nSo sorry I was wrong with the suggested test case to test this issue. To test this you need to e.g. set max-servers to 1 and request a nodeset with two nodes.",
      "parentUuid": "5f7c97a3_6c754833",
      "range": {
        "startLine": 331,
        "startChar": 0,
        "endLine": 333,
        "endChar": 73
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_f8e7d70e",
        "filename": "nodepool/tests/test_launcher.py",
        "patchSetId": 10
      },
      "lineNbr": 1440,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "I think these are two test cases in one (ignore \u003d True and False). To be easier to read I think this could be split into two test cases.",
      "range": {
        "startLine": 1440,
        "startChar": 8,
        "endLine": 1440,
        "endChar": 34
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_187173e0",
        "filename": "nodepool/tests/test_launcher.py",
        "patchSetId": 10
      },
      "lineNbr": 1440,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "No problem - can do that.",
      "parentUuid": "5f7c97a3_f8e7d70e",
      "range": {
        "startLine": 1440,
        "startChar": 8,
        "endLine": 1440,
        "endChar": 34
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_7840a707",
        "filename": "nodepool/tests/test_launcher.py",
        "patchSetId": 10
      },
      "lineNbr": 1492,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "We need to request a further node here that exceeds the configured pool max-foo and check if it gets to the zk.PENDING state to indicate that the pool max values still work. Regarding the pending state you can look at test_over_quota for a test example.",
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_982b439a",
        "filename": "nodepool/tests/test_launcher.py",
        "patchSetId": 10
      },
      "lineNbr": 1492,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "Good idea, will do thanks.",
      "parentUuid": "5f7c97a3_7840a707",
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_d8c3fb57",
        "filename": "releasenotes/notes/ignore-provider-quota-aa19e7a7271ee106.yaml",
        "patchSetId": 10
      },
      "lineNbr": 4,
      "author": {
        "id": 16068
      },
      "writtenOn": "2018-07-11T16:04:41Z",
      "side": 1,
      "message": "The backticks look broken.",
      "range": {
        "startLine": 4,
        "startChar": 32,
        "endLine": 4,
        "endChar": 59
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "5f7c97a3_38a9cf24",
        "filename": "releasenotes/notes/ignore-provider-quota-aa19e7a7271ee106.yaml",
        "patchSetId": 10
      },
      "lineNbr": 4,
      "author": {
        "id": 6816
      },
      "writtenOn": "2018-07-11T16:22:49Z",
      "side": 1,
      "message": "Good spot, thanks.",
      "parentUuid": "5f7c97a3_d8c3fb57",
      "range": {
        "startLine": 4,
        "startChar": 32,
        "endLine": 4,
        "endChar": 59
      },
      "revId": "4c3277d718c6d359c7f27d2664d26cd49b00299e",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    }
  ]
}