{
  "comments": [
    {
      "key": {
        "uuid": "9fdfeff1_55e2a5a1",
        "filename": "nodepool/driver/runc/handler.py",
        "patchSetId": 22
      },
      "lineNbr": 102,
      "author": {
        "id": 13413
      },
      "writtenOn": "2019-02-15T12:33:22Z",
      "side": 1,
      "message": "This looks OK to me who has not checked the rest of Zuul on how these methods are used, but they do not work for me. I am aiming for max. one container per each \"pool\", and this is the config that I am using:\n\n # This file is generated by Ansible\n #  DO NOT EDIT THIS FILE BY HAND -- YOUR CHANGES WILL BE OVERWRITTEN\n #\n ---\n images-dir: /opt/nodepool/images\n \n zookeeper-servers:\n   - host: localhost\n     port: 2181\n \n diskimages: []\n \n labels:\n   - name: f29\n     min-ready: 0\n     max-ready: 0\n \n providers:\n   - name: potemkin-runc\n     driver: runc\n     pools:\n       - name: ci-f29-potemkin05-vm.vesnicky.cesnet.cz\n         host-key: \u0027ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHuMVGaDJLxNlUBDQ05zM2/aEtvBoFL/quv1YK0oEkgJ\u0027\n         max-servers: 1\n         labels:\n           - name: f29\n             username: ci\n             path: /containers/f29\n       - name: ci-f29-potemkin06-vm.vesnicky.cesnet.cz\n         host-key: \u0027ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIHH329IczhjR/WGSHTLtMENVP5rrKblyLZr3HWOkwgIY\u0027\n         max-servers: 1\n         labels:\n           - name: f29\n             username: ci\n             path: /containers/f29\n       - name: ci-f29-potemkin07-vm.vesnicky.cesnet.cz\n         host-key: \u0027ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIEJTPjfpP0z3mKwz8iL8rfUwN+G10Ucs+XUHrXd9INWu\u0027\n         max-servers: 1\n         labels:\n           - name: f29\n             username: ci\n             path: /containers/f29\n       - name: ci-f29-potemkin08-vm.vesnicky.cesnet.cz\n         host-key: \u0027ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJL8dD6/MD4pDKxxqgM7o24PtBDCsCIx6fUvOLMb0Z+q\u0027\n         max-servers: 1\n         labels:\n           - name: f29\n             username: ci\n             path: /containers/f29\n\nWhen I submitted a patch which requires four jobs, three of them were launched at one node:\n\n $ ssh root@nodepool.gerrit.cesnet.cz nodepool list\n +------------+---------------+-------+-------------------------------+-------------+--------------------------------+--------+-------------+--------+\n | ID         | Provider      | Label | Server ID                     | Public IPv4 | IPv6                           | State  | Age         | Locked |\n +------------+---------------+-------+-------------------------------+-------------+--------------------------------+--------+-------------+--------+\n | 0000000056 | potemkin-runc | f29   | 0000000056-f29-200-0000000053 | None        | 2001:718:1:28:5054:ff:fece:501 | in-use | 00:00:00:16 | locked |\n | 0000000058 | potemkin-runc | f29   | 0000000058-f29-200-0000000055 | None        | 2001:718:1:28:5054:ff:fece:801 | in-use | 00:00:00:16 | locked |\n | 0000000057 | potemkin-runc | f29   | 0000000057-f29-200-0000000054 | None        | 2001:718:1:28:5054:ff:fece:501 | in-use | 00:00:00:16 | locked |\n | 0000000059 | potemkin-runc | f29   | 0000000059-f29-200-0000000056 | None        | 2001:718:1:28:5054:ff:fece:501 | in-use | 00:00:00:16 | locked |\n +------------+---------------+-------+-------------------------------+-------------+--------------------------------+--------+-------------+--------+\n\n(The last part of the IPv6 address identifies the runc hypervisor in this case.)\n\nWhat am I doing wrong?",
      "range": {
        "startLine": 91,
        "startChar": 0,
        "endLine": 102,
        "endChar": 20
      },
      "revId": "0d67bc8ea0d6fc99ba1c57f32b165eaf5a204232",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9fdfeff1_bc0acc86",
        "filename": "nodepool/driver/runc/handler.py",
        "patchSetId": 22
      },
      "lineNbr": 102,
      "author": {
        "id": 9311
      },
      "writtenOn": "2019-02-16T00:02:49Z",
      "side": 1,
      "message": "There is an issue indeed. The max-servers configuration is actually provider scope (not pool scope), but the code is using the default pool\u0027s max-server which is inf. The fix would be to make the max-server\u0027s pool scope in the runc/config.py.\n\nAlso, not sure why you got nodes with \"max-ready: 0\".",
      "parentUuid": "9fdfeff1_55e2a5a1",
      "range": {
        "startLine": 91,
        "startChar": 0,
        "endLine": 102,
        "endChar": 20
      },
      "revId": "0d67bc8ea0d6fc99ba1c57f32b165eaf5a204232",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9fdfeff1_d5fada41",
        "filename": "nodepool/driver/runc/handler.py",
        "patchSetId": 22
      },
      "lineNbr": 102,
      "author": {
        "id": 13413
      },
      "writtenOn": "2019-02-19T11:41:04Z",
      "side": 1,
      "message": "I double-checked the config propagation, and it appears to work. When I add extra debugging output to RuncNodeRequestHandler.hasRemainingQuota, the pool.max_servers is correctly set to 1, but pool.containers remains empty.\n\nI checked this with a change that requires 9 jobs and a pool config containing 4 pools (runc hypervisors), each with max-servers: 1.\n\nI also tried queueing two changes at once, each again resulting in 9 jobs. This resulted in 18 containers on 4 hypervisors.\n\nThis looks like a time-of-check, time-of-use issue to me. Stuff gets added into pool.containers only after they are really started via RuncProvider.createContainer(). Perhaps there should be some locking around that? How do other providers handle this?",
      "parentUuid": "9fdfeff1_bc0acc86",
      "range": {
        "startLine": 91,
        "startChar": 0,
        "endLine": 102,
        "endChar": 20
      },
      "revId": "0d67bc8ea0d6fc99ba1c57f32b165eaf5a204232",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "9fdfeff1_7b599d33",
        "filename": "nodepool/driver/runc/handler.py",
        "patchSetId": 22
      },
      "lineNbr": 102,
      "author": {
        "id": 13413
      },
      "writtenOn": "2019-02-19T12:57:44Z",
      "side": 1,
      "message": "...and this is because `self.provider` is a RuncProviderConfig, and `pool` is an instance of RuncPool which is again just a config class. The container instances are managed elsewhere, within a RuncProvider instance. I\u0027ll try to see if I can find out how to access that one.",
      "parentUuid": "9fdfeff1_d5fada41",
      "range": {
        "startLine": 91,
        "startChar": 0,
        "endLine": 102,
        "endChar": 20
      },
      "revId": "0d67bc8ea0d6fc99ba1c57f32b165eaf5a204232",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543",
      "unresolved": false
    }
  ]
}