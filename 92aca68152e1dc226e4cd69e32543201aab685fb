{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "40f3f878_355dfa01",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 2
      },
      "lineNbr": 0,
      "author": {
        "id": 33934
      },
      "writtenOn": "2022-03-10T17:26:54Z",
      "side": 1,
      "message": "Is this what you had in mind?",
      "revId": "92aca68152e1dc226e4cd69e32543201aab685fb",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8e6308b9_307f2432",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 2
      },
      "lineNbr": 302,
      "author": {
        "id": 4146
      },
      "writtenOn": "2022-03-14T16:12:52Z",
      "side": 1,
      "message": "I think this long sleep is why the testing fails. We probably check states before this sleep is completed.\n\nAs far as implementing the sleep I wonder if we need to do a hybrid between the current pause and the sleeping with retries to ensure we don\u0027t end up with a provider that grabs all the node requests but they fail for a long time due to quota errors. Something like having the first node to hit a quota error continue to retry but pause all other requests? That should provide the request pressure you want against the API while also avoiding having a single provider take all requests then fail to service them due to quotas.",
      "revId": "92aca68152e1dc226e4cd69e32543201aab685fb",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "73a226b5_2520071a",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 2
      },
      "lineNbr": 302,
      "author": {
        "id": 33934
      },
      "writtenOn": "2022-03-22T15:29:23Z",
      "side": 1,
      "message": "I did some digging, and I don\u0027t think it would take all the requests. My understanding from reading the code is that only the requests that race in `_waitForNodeSet` checking the quota would enter this loop. The requests after that will automatically pause the handler (especially since we invalidate the quota cache). I don\u0027t think any \"pausing\" is necessary here since the code in `_waitForNodeSet` handles that already; the only thing we could *maybe* do here is try and detect if multiple threads are in this loop because of quota reasons and kick all but one out.... but this seems complex and error prone so I\u0027d rather not unless it\u0027s a huge problem to have a few nodes in the loop until quota is satisfied (realizing that it won\u0027t consume all nodes because of the reasons stated above).\n\n\nIf my understanding is incorrect, please let me know :)",
      "parentUuid": "8e6308b9_307f2432",
      "revId": "92aca68152e1dc226e4cd69e32543201aab685fb",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "af888a63_92659560",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 2
      },
      "lineNbr": 302,
      "author": {
        "id": 16068
      },
      "writtenOn": "2022-03-23T20:07:10Z",
      "side": 1,
      "message": "We also need to take into consideration that we raise here for a reason which is to release the node request and give a different provider on a different cloud the chance to fulfill the node request. The reason for this is that otherwise we\u0027d block the request until it can be satisfied while having free resources on a different cloud. This can be especially an issue on small multi-cloud systems.",
      "parentUuid": "73a226b5_2520071a",
      "revId": "92aca68152e1dc226e4cd69e32543201aab685fb",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "cb2a62a3_70872e48",
        "filename": "nodepool/driver/openstack/handler.py",
        "patchSetId": 2
      },
      "lineNbr": 302,
      "author": {
        "id": 33934
      },
      "writtenOn": "2022-03-24T13:18:33Z",
      "side": 1,
      "message": "Right, I\u0027m not sure how to reconcile those two; our setup needs the quota pressure to get the quote increased, but another setup might want to fall through to another provider... I\u0027m not sure how to saitisfy both constraints since they seem contradictory; I hate to add a configuration option to control the behavior, but it\u0027s all I can really think of right know.",
      "parentUuid": "af888a63_92659560",
      "revId": "92aca68152e1dc226e4cd69e32543201aab685fb",
      "serverId": "4a232e18-c5a9-48ee-94c0-e04e7cca6543"
    }
  ]
}